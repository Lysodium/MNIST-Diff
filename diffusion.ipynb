{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, Softplus\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "import tqdm.notebook as tqdm\n",
    "from huggingface_hub import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: (x*2)-1)])\n",
    "                                 #transforms.Normalize(mean=0.5, std=0.5)])\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./MNIST', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./MNIST', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            Linear(input_dim, 512),\n",
    "            ReLU(),\n",
    "            Linear(512, 512),\n",
    "            ReLU(),\n",
    "            Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "class Diffusion(nn.Module):\n",
    "    def __init__(self, data_dim, T=300, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = MLP(data_dim + 1, data_dim)\n",
    "        self.data_dim = data_dim\n",
    "        self.T = T\n",
    "        self.beta = linear_beta_schedule(T).to(self.device)\n",
    "        # constants for sampling\n",
    "        self._init_scalars()\n",
    "\n",
    "    def _init_scalars(self):\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.cumprod_alpha = torch.cumprod(self.alpha, dim=0).to(self.device)\n",
    "        alphas_cumprod_prev = torch.nn.functional.pad(self.cumprod_alpha[:-1], (1, 0), value=1)\n",
    "        self.posterior_variance = self.beta * alphas_cumprod_prev / (1 - self.cumprod_alpha)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        input = torch.cat([x, t], dim=1)\n",
    "        return self.model(input)\n",
    "\n",
    "    def sample_q_t(self, x_0, t):\n",
    "        \" forward noising step \"\n",
    "        noise = torch.randn_like(x_0).to(self.device)\n",
    "        # scalars for sampling\n",
    "        sqrt_alpha_cumprod = torch.sqrt(self.cumprod_alpha[t])\n",
    "        sqrt_one_minus_alpha_cumprod = torch.sqrt(1 - self.cumprod_alpha[t])\n",
    "        # sample\n",
    "        x_t = sqrt_alpha_cumprod * x_0 +  sqrt_one_minus_alpha_cumprod * noise\n",
    "        return x_t\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_p_t(self, x_t_prev, t):\n",
    "        \" backward denoising step \"\n",
    "        # scalars for sampling\n",
    "        sqrt_recip_alpha_cumprod = torch.sqrt(1 / self.cumprod_alpha[t])\n",
    "        one_minus_alpha_cumprod = 1 - self.cumprod_alpha[t]\n",
    "        scaling = self.beta[t] / torch.sqrt(one_minus_alpha_cumprod)\n",
    "        # sample\n",
    "        pred = self.forward(x_t_prev, t)\n",
    "\n",
    "        print(x_t_prev.mean(), x_t_prev.std())\n",
    "        print((x_t_prev - scaling * pred).mean())\n",
    "        print()\n",
    "        mean = sqrt_recip_alpha_cumprod * (x_t_prev - scaling * pred)\n",
    "        std = torch.sqrt(self.posterior_variance[t])\n",
    "        \n",
    "        noise = torch.randn_like(x_t_prev) if t > 0 else torch.zeros_like(x_t_prev)\n",
    "        x_t = mean + std * noise.to(self.device)\n",
    "        return x_t\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self):\n",
    "        \" sample from the model \"\n",
    "        x_0 = torch.randn(size=(1, self.data_dim)).to(self.device)\n",
    "        x_t = x_0\n",
    "        for t in reversed(range(self.T)):\n",
    "            t = torch.tensor([t]).unsqueeze(-1).to(self.device)\n",
    "            x_t = self.sample_p_t(x_t, t)\n",
    "        return x_t\n",
    "\n",
    "    \n",
    "class DiffusionTrainer:\n",
    "    def __init__(self, diffusion, optimizer, device='cuda'):\n",
    "        self.diffusion = diffusion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "    def train_epoch(self, batch):\n",
    "        # sample random t for every batch element\n",
    "        t = torch.randint(\n",
    "            0, \n",
    "            self.diffusion.T, \n",
    "            (batch.shape[0],)\n",
    "        ).unsqueeze(-1).to(self.device)\n",
    "        # sample x_0 ~ q(x_0)\n",
    "        x_0 = torch.randn_like(batch).to(self.device)\n",
    "        # sample x_t ~ q(x_t|x_0)\n",
    "        x_t = self.diffusion.sample_q_t(x_0, t)\n",
    "        # predict noise\n",
    "        noise = self.diffusion(x_t, t)\n",
    "        # compute loss\n",
    "        loss = torch.nn.functional.mse_loss(noise, batch)\n",
    "        # optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, trainloader, epochs=10):\n",
    "        self.diffusion.train()\n",
    "        for _ in range(epochs):\n",
    "            for x, _ in tqdm(trainloader):\n",
    "                x = x.reshape(x.shape[0], -1).to(self.device)\n",
    "                self.train_epoch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward process\n",
    "diffusion = Diffusion(784).cuda()\n",
    "image = next(iter(trainloader))[0][0].reshape((1, 784)).to('cuda')\n",
    "for t in range(0, 300, 25):\n",
    "    x_t = diffusion.sample_q_t(image, t)\n",
    "    plt.figure()\n",
    "    plt.imshow(x_t.view(28,28).detach().cpu(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "diffusion = Diffusion(784).cuda()\n",
    "# optimizer = Adam(diffusion.parameters(), lr=1e-3)\n",
    "# trainer = DiffusionTrainer(diffusion, optimizer)\n",
    "# trainer.train(trainloader, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = diffusion.sample()\n",
    "plt.imshow(sample.view(28,28).detach().cpu(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
